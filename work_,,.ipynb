{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MKOCj_fcDXCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHNklCAAhDnh",
        "outputId": "fe21f086-2de6-4ded-eb81-550045aa34c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas matplotlib opencv-python pydicom scikit-image tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JENTOclBhG_R",
        "outputId": "50afc533-fc42-44db-f197-c44718700da9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the path Should be update"
      ],
      "metadata": {
        "id": "FBAwVh3KhLff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "\n",
        "# Path to the dataset folder in Google Drive\n",
        "DATASET_PATH = \"/content/drive/MyDrive/kaggle_3m\"  # Update this if necessary\n",
        "\n",
        "# Check dataset structure\n",
        "print(\"Folders in dataset:\", os.listdir(DATASET_PATH)[:5])  # Show first 5 patient folders\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHrNWkBhIdJ",
        "outputId": "4f84764f-0f1d-4f73-e902-0984518dfa25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders in dataset: ['README.md', 'data.csv', 'TCGA_HT_8106_19970727', 'TCGA_HT_8114_19981030', 'TCGA_HT_8111_19980330']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # Optional for progress bar\n",
        "\n",
        "# Define dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/kaggle_3m\"\n",
        "IMG_SIZE = (256, 256)  # Resize target\n",
        "\n",
        "def load_dataset(dataset_path, img_size=(256, 256)):\n",
        "    images = []\n",
        "    masks = []\n",
        "    filenames = []\n",
        "\n",
        "    # Go through each patient folder\n",
        "    for folder_name in tqdm(sorted(os.listdir(dataset_path))):\n",
        "        folder_path = os.path.join(dataset_path, folder_name)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue  # Skip files like README.md or CSVs\n",
        "\n",
        "        # List all .tif files in the folder\n",
        "        files = sorted(os.listdir(folder_path))\n",
        "        img_files = [f for f in files if f.endswith(\".tif\") and \"_mask\" not in f]\n",
        "        mask_files = [f for f in files if f.endswith(\"_mask.tif\")]\n",
        "\n",
        "        # Build dict for pairing\n",
        "        img_dict = {f.replace(\".tif\", \"\"): f for f in img_files}\n",
        "        mask_dict = {f.replace(\"_mask.tif\", \"\"): f for f in mask_files}\n",
        "\n",
        "        # Match images and masks by their base name\n",
        "        common_keys = set(img_dict.keys()) & set(mask_dict.keys())\n",
        "\n",
        "        for key in sorted(common_keys):\n",
        "            img_path = os.path.join(folder_path, img_dict[key])\n",
        "            mask_path = os.path.join(folder_path, mask_dict[key])\n",
        "\n",
        "            # Load the image in grayscale or color as needed\n",
        "            image = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Use grayscale if needed\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if image is None or mask is None:\n",
        "                continue  # Skip if loading fails\n",
        "\n",
        "            # Resize both\n",
        "            image = cv2.resize(image, img_size)\n",
        "            mask = cv2.resize(mask, img_size)\n",
        "\n",
        "            # Normalize image and threshold mask\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "            mask = (mask > 0).astype(np.uint8)  # Convert mask to binary\n",
        "\n",
        "            images.append(image)\n",
        "            masks.append(mask)\n",
        "            filenames.append(img_dict[key])\n",
        "\n",
        "    images = np.array(images)\n",
        "    masks = np.array(masks)[..., np.newaxis]  # Add channel dim for masks (N, H, W, 1)\n",
        "    return images, masks, filenames\n",
        "\n",
        "# Load the dataset\n",
        "images, masks, filenames = load_dataset(DATASET_PATH)\n",
        "\n",
        "# Display dataset info\n",
        "print(\"✅ Total Scans Loaded:\", len(images))\n",
        "print(\"Image shape:\", images.shape)\n",
        "print(\"Mask shape:\", masks.shape)\n",
        "print(\"Unique mask values (sample):\", np.unique(masks[0]))\n",
        "\n",
        "# Visualize a few samples\n",
        "def show_samples(images, masks, filenames, num=4):\n",
        "    import random\n",
        "    indices = random.sample(range(len(images)), num)\n",
        "    plt.figure(figsize=(10, num * 3))\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(num, 2, i*2 + 1)\n",
        "        plt.imshow(images[idx])\n",
        "        plt.title(f\"Image: {filenames[idx]}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(num, 2, i*2 + 2)\n",
        "        plt.imshow(masks[idx].squeeze(), cmap=\"gray\")\n",
        "        plt.title(\"Mask\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T90rNHma_hMM",
        "outputId": "c613714e-531d-4c31-8282-4149aadf508b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [20:29<00:00, 10.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total Scans Loaded: 3929\n",
            "Image shape: (3929, 256, 256, 3)\n",
            "Mask shape: (3929, 256, 256, 1)\n",
            "Unique mask values (sample): [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mask pixel sum (first 10):\", [np.sum(mask) for mask in masks[:10]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6l12x3uyfOP",
        "outputId": "493b2eca-dbf9-453a-f041-c11e56f0fc84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask pixel sum (first 10): [np.uint64(0), np.uint64(0), np.uint64(1426), np.uint64(2646), np.uint64(2765), np.uint64(2877), np.uint64(1952), np.uint64(1828), np.uint64(811), np.uint64(74)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training (80%), validation (10%), and test (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training set:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv9xZaoy3UJ",
        "outputId": "96d3e66d-1b11-4421-b821-ae9afb0b01ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (3143, 256, 256, 3) (3143, 256, 256, 1)\n",
            "Validation set: (393, 256, 256, 3) (393, 256, 256, 1)\n",
            "Test set: (393, 256, 256, 3) (393, 256, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a Python generator function\n",
        "def data_generator(X, y):\n",
        "    for img, mask in zip(X, y):\n",
        "        yield img, mask\n",
        "\n",
        "# Use the generator to create a TensorFlow dataset\n",
        "def create_tf_dataset(X, y, batch_size=16):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(X, y),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.uint8)\n",
        "        )\n",
        "    )\n",
        "    dataset = dataset.shuffle(500).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create datasets without consuming excessive RAM\n",
        "train_dataset = create_tf_dataset(X_train, y_train)\n",
        "val_dataset = create_tf_dataset(X_val, y_val)\n",
        "test_dataset = create_tf_dataset(X_test, y_test)\n",
        "\n",
        "print(\"✅ TF Datasets created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-1Kbdkty9wX",
        "outputId": "0c093fe0-8343-4641-f9c6-2bdd604b7be9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TF Datasets created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D U-net model"
      ],
      "metadata": {
        "id": "43y1pWuYihjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Edge Detection Module using Sobel Filter\n",
        "class EdgeDetection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EdgeDetection, self).__init__()\n",
        "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        self.weight_x = nn.Parameter(sobel_x, requires_grad=False)\n",
        "        self.weight_y = nn.Parameter(sobel_y, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        edge_x = F.conv2d(x, self.weight_x, padding=1)\n",
        "        edge_y = F.conv2d(x, self.weight_y, padding=1)\n",
        "        edges = torch.sqrt(edge_x ** 2 + edge_y ** 2)\n",
        "        return edges\n",
        "\n",
        "# U-Net Block with Edge Features\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# ED-U-Net Model\n",
        "class ED_UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=5):\n",
        "        super(ED_UNet, self).__init__()\n",
        "        self.edge_detector = EdgeDetection()\n",
        "\n",
        "        self.enc1 = DoubleConv(in_channels + 1, 64)  # Extra channel for edge map\n",
        "        self.enc2 = DoubleConv(64, 128)\n",
        "        self.enc3 = DoubleConv(128, 256)\n",
        "        self.enc4 = DoubleConv(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec1 = DoubleConv(1024, 512)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec2 = DoubleConv(512, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec3 = DoubleConv(256, 128)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec4 = DoubleConv(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        edges = self.edge_detector(x[:, :1, :, :])  # Apply edge detection to first channel\n",
        "        x = torch.cat([x, edges], dim=1)  # Concatenate edges as extra channel\n",
        "\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        up1 = self.up1(bottleneck)\n",
        "        dec1 = self.dec1(torch.cat([up1, enc4], dim=1))\n",
        "\n",
        "        up2 = self.up2(dec1)\n",
        "        dec2 = self.dec2(torch.cat([up2, enc3], dim=1))\n",
        "\n",
        "        up3 = self.up3(dec2)\n",
        "        dec3 = self.dec3(torch.cat([up3, enc2], dim=1))\n",
        "\n",
        "        up4 = self.up4(dec3)\n",
        "        dec4 = self.dec4(torch.cat([up4, enc1], dim=1))\n",
        "\n",
        "        return self.final_conv(dec4)\n",
        "\n",
        "# Initialize Model\n",
        "model = ED_UNet(in_channels=3, out_channels=5)  # 5 classes for multi-class segmentation\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEaQCxpFij1e",
        "outputId": "9b1d7b0f-2c79-4802-c4ed-512500f2448b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ED_UNet(\n",
            "  (edge_detector): EdgeDetection()\n",
            "  (enc1): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc2): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc3): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (enc4): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bottleneck): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec1): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec2): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec3): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec4): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (final_conv): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YbqCTiv6bRV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Convert TensorFlow dataset into PyTorch Dataset\n",
        "class BrainSegmentationDataset(Dataset):\n",
        "    def __init__(self, images, masks):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx].transpose(2, 0, 1)  # (H, W, C) → (C, H, W)\n",
        "        mask = self.masks[idx].squeeze()  # Remove extra dimension if needed\n",
        "\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        mask = torch.tensor(mask, dtype=torch.long)  # For multi-class segmentation\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Convert datasets\n",
        "train_dataset = BrainSegmentationDataset(X_train, y_train)\n",
        "val_dataset = BrainSegmentationDataset(X_val, y_val)\n",
        "test_dataset = BrainSegmentationDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"✅ PyTorch DataLoader created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPPDXqh3BBNc",
        "outputId": "bf94abac-acbb-4acc-ab0f-302c33fcfa20"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PyTorch DataLoader created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here below i have added import torch.nn as np"
      ],
      "metadata": {
        "id": "57Ze4eV8bYeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Dice Loss (for better segmentation)\n",
        " class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "    def forward(self, preds, targets, smooth=1.0):\n",
        "        preds = torch.softmax(preds, dim=1)  # Convert logits to probabilities\n",
        "        targets = torch.nn.functional.one_hot(targets, num_classes=5).permute(0, 3, 1, 2)  # One-hot encode masks\n",
        "        intersection = torch.sum(preds * targets, dim=(2, 3))\n",
        "        union = torch.sum(preds + targets, dim=(2, 3))\n",
        "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
        "        return 1 - dice.mean()\n",
        " # Loss function (Dice + CrossEntropy)\n",
        " criterion = lambda preds, targets: 0.5 * nn.CrossEntropyLoss()(preds, targets) + 0.5 * DiceLoss()(preds, targets)"
      ],
      "metadata": {
        "id": "kkeOwlckBGxn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple model (replace with your actual model)\n",
        "class UNet(nn.Module):  # Example segmentation model\n",
        "    def __init__(self, in_channels=3, num_classes=5):\n",
        "        super(UNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = UNet()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "ivLekBj6qedW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    print(\"✅ Training complete!\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n"
      ],
      "metadata": {
        "id": "yYXE_ZUJBsav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ED_UNet(in_channels=3, out_channels=1).to(device)\n",
        "model.load_state_dict(torch.load(\"ed_unet_brain_segmentation.pth\", map_location=device))\n",
        "model.eval()  # Set to inference mode\n",
        "\n",
        "# Convert to FP16 (Half-Precision)\n",
        "model.half()\n",
        "\n",
        "# Example input tensor for testing\n",
        "example_input = torch.randn(1, 3, 256, 256, dtype=torch.half, device=device)\n",
        "\n",
        "# Run inference to check\n",
        "with torch.no_grad():\n",
        "    output = model(example_input)\n",
        "\n",
        "print(\"Model successfully converted to FP16 & tested!\")\n"
      ],
      "metadata": {
        "id": "UcsYEj4ukQsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model to TorchScript (Optimized for Deployment)\n",
        "traced_model = torch.jit.trace(model, example_input)\n",
        "torch.jit.save(traced_model, \"ed_unet_quantized.pt\")\n",
        "\n",
        "print(\"TorchScript model saved: ed_unet_quantized.pt\")\n"
      ],
      "metadata": {
        "id": "y2jUCwIgkSdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Convert model to ONNX format\n",
        "onnx_model_path = \"ed_unet_quantized.onnx\"\n",
        "torch.onnx.export(\n",
        "    model, example_input, onnx_model_path,\n",
        "    export_params=True, opset_version=11,\n",
        "    do_constant_folding=True, input_names=['input'], output_names=['output']\n",
        ")\n",
        "\n",
        "# Check ONNX Model\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "print(f\"ONNX Model saved: {onnx_model_path}\")\n"
      ],
      "metadata": {
        "id": "LHcIPVqIkUCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation"
      ],
      "metadata": {
        "id": "Z5mZuxZyk6mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load optimized model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.jit.load(\"ed_unet_quantized.pt\").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to preprocess image for inference\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.0\n",
        "    image = cv2.resize(image, (256, 256))\n",
        "    image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).to(device)  # Convert to (1, C, H, W)\n",
        "    return image.half()  # Use FP16 for optimized inference\n",
        "\n",
        "# Inference Function\n",
        "def predict_mask(image_path):\n",
        "    image = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        pred_mask = model(image).squeeze().cpu().numpy()  # Convert back to NumPy\n",
        "    pred_mask = (pred_mask > 0.5).astype(np.uint8)  # Apply thresholding\n",
        "    return pred_mask\n"
      ],
      "metadata": {
        "id": "sP9EwWOEk8bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Score & IoU Calculation\n",
        "def dice_score(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    return (2. * intersection) / (np.sum(y_true) + np.sum(y_pred) + 1e-7)\n",
        "\n",
        "def iou_score(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
        "    return intersection / (union + 1e-7)\n",
        "\n",
        "# Evaluate on a Test Image\n",
        "mask_true = cv2.imread(\"test_mask.tif\", cv2.IMREAD_GRAYSCALE) / 255.0  # Ground truth mask\n",
        "mask_pred = predict_mask(\"test_image.tif\")  # Predicted mask\n",
        "\n",
        "print(f\"Dice Score: {dice_score(mask_true, mask_pred):.4f}\")\n",
        "print(f\"IoU Score: {iou_score(mask_true, mask_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "1LbQ1hXnk-Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract tumor pixel intensities from different patient groups\n",
        "tumor_group1 = mask_pred[mask_true == 1].flatten()  # Example group 1\n",
        "tumor_group2 = mask_pred[mask_true == 0].flatten()  # Example group 2\n",
        "\n",
        "# Perform t-test (Compare segmented regions)\n",
        "t_stat, p_value = ttest_ind(tumor_group1, tumor_group2, equal_var=False)\n",
        "\n",
        "print(f\"T-Test Statistic: {t_stat:.4f}, P-Value: {p_value:.4e}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"Significant difference in tumor segmentations between groups (p < 0.05).\")\n",
        "else:\n",
        "    print(\"No significant difference in tumor segmentations.\")\n"
      ],
      "metadata": {
        "id": "pFHlGfaTk_fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Heatmap of Predicted Mask\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(mask_pred, cmap=\"jet\", alpha=0.6)\n",
        "plt.title(\"Heatmap of Segmented Region\")\n",
        "plt.show()\n",
        "\n",
        "# Volumetric Plot (Histogram of segmented areas)\n",
        "plt.hist(mask_pred.flatten(), bins=50, alpha=0.7, color=\"blue\", label=\"Segmented Pixels\")\n",
        "plt.legend()\n",
        "plt.title(\"Tumor Volume Distribution\")\n",
        "plt.xlabel(\"Intensity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DpS5Go0rlA3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}